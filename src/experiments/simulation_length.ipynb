{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'src' as root folder, to find other modules in the project\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "from simulator.swing_equation import swing_equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kit/iai/ik6649/Data-Driven-Bayesian-Parameter-Estimation-with-Neural-Networks-1/.venv/lib64/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# external imports\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sbi.inference import SNPE, simulate_for_sbi, prepare_for_sbi\n",
    "from sbi.utils import BoxUniform, posterior_nn\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import kurtosis, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the experimental setup variables\n",
    "num_simulations = 10000\n",
    "dts = [0.1, 0.05, 0.01]\n",
    "T = 10\n",
    "param_names = ['c1', 'c2', 'P0', 'P1', 'epsilon']\n",
    "\n",
    "# Define output directories for results\n",
    "dir_path_plots = '../../results/simulation_length/plots/'\n",
    "dir_path_tables = '../../results/simulation_length/csv/'\n",
    "\n",
    "# Create the directories\n",
    "os.makedirs(dir_path_plots, exist_ok=True)\n",
    "os.makedirs(dir_path_tables, exist_ok=True)\n",
    "\n",
    "# Define the prior distribution for the parameters\n",
    "prior_low = torch.tensor([0.1, 1.0, 0.1, 40.0, 0.1])\n",
    "prior_high = torch.tensor([10.0, 100.0, 10.0, 70.0, 10.0])\n",
    "\n",
    "prior = BoxUniform(low=prior_low, high=prior_high)\n",
    "\n",
    "# Simulation parameters\n",
    "simulator = swing_equation\n",
    "proposal = prior\n",
    "num_simulations = 20000\n",
    "num_workers = 20\n",
    "simulation_batch_size = 1\n",
    "seed = None\n",
    "show_progress_bar = True\n",
    "\n",
    "# Density estimator parameters\n",
    "model = 'maf'\n",
    "hidden_features = 50\n",
    "num_transforms = 10\n",
    "z_score_x = 'independent'\n",
    "z_score_theta = 'independent'\n",
    "num_bins = 10\n",
    "num_components = 10\n",
    "\n",
    "# Define the true parameters for training and evaluation\n",
    "theta_true = torch.tensor([4.0, 40.0, 3.0, 60.0, 7.0])\n",
    "\n",
    "# Define the simulators with different timesteps and observations from them\n",
    "sim_ops = []\n",
    "\n",
    "for dt in dts:\n",
    "        \n",
    "    # Wrap the simulator to allow for timestep change\n",
    "    def wrap_simulator(parameters):\n",
    "        return swing_equation(parameters, dt=dt, T=T)\n",
    "    \n",
    "    # create simulators and corresponding observations, and total length of the simulation\n",
    "    sim_ops.append((wrap_simulator, wrap_simulator(theta_true), T / dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running 20000 simulations in 20000 batches.:   0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Run inference and collect resulting posteriors\n",
    "posteriors = []\n",
    "\n",
    "for simulator, observation, t in sim_ops:\n",
    "\n",
    "    # Wrap the simulator function for compatibility with SBI\n",
    "    simulator, prior = prepare_for_sbi(simulator, prior)\n",
    "\n",
    "    # Instantiate the neural density estimator\n",
    "    neural_posterior = posterior_nn(\n",
    "        model=model, \n",
    "        hidden_features=hidden_features, \n",
    "        num_transforms=num_transforms, \n",
    "        num_bins=num_bins, \n",
    "        num_components=num_components, \n",
    "        z_score_theta=z_score_theta, \n",
    "        z_score_x=z_score_x\n",
    "    )\n",
    "\n",
    "    # Set up the inference procedure with the SNPE procedure\n",
    "    inference = SNPE(prior=prior)\n",
    "\n",
    "    # Run the inference procedure to generate samples and corresponding simulated data points\n",
    "    theta, x = simulate_for_sbi(\n",
    "        simulator=simulator, \n",
    "        proposal=proposal, \n",
    "        num_simulations=num_simulations, \n",
    "        num_workers=num_workers, \n",
    "        simulation_batch_size=simulation_batch_size, \n",
    "        seed=seed, \n",
    "        show_progress_bar=show_progress_bar\n",
    "    )\n",
    "\n",
    "    # Train the neural density estimator\n",
    "    density_estimator = inference.append_simulations(theta, x).train()\n",
    "\n",
    "    # Build the posterior for the given parameters\n",
    "    posterior = inference.build_posterior(density_estimator)\n",
    "    \n",
    "    posterior.set_default_x(observation)\n",
    "\n",
    "    # add (posterior, simulation length) to the posteriors list\n",
    "    posteriors.append((posterior, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m param_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(timesteps):\n\u001b[0;32m     15\u001b[0m     \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Extract posterior distributions for current T\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     posterior_samples \u001b[38;5;241m=\u001b[39m \u001b[43mposteriors\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msample((\u001b[38;5;241m10000\u001b[39m,), show_progress_bars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Create separate figures for each type of plot\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     fig1, axes1 \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(param_names), figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for posterior, t in posteriors:\n",
    "    \n",
    "    # Extract posterior distributions for current T\n",
    "    posterior_samples = posterior.sample((10000,), show_progress_bars=False)\n",
    "\n",
    "    # Create separate figures for each type of plot\n",
    "    fig1, axes1 = plt.subplots(1, len(param_names), figsize=(20, 4))\n",
    "    fig2, axes2 = plt.subplots(1, len(param_names), figsize=(20, 4))\n",
    "    fig3, axes3 = plt.subplots(1, len(param_names), figsize=(20, 4))\n",
    "\n",
    "    # Plot each parameter for current length\n",
    "    for j in range(len(param_names)):\n",
    "        parameter_samples = posterior_samples[:, j]\n",
    "        \n",
    "        # Histogram\n",
    "        axes1[j].hist(parameter_samples, bins=30, density=True, alpha=0.7, label='Inferred')\n",
    "        axes1[j].axvline(x=theta_true[j], color='red', linestyle='--', label='True')\n",
    "\n",
    "        axes1[j].set_title(param_names[j] + ' - Histogram')\n",
    "        axes1[j].legend()\n",
    "        \n",
    "        # Scatter plot\n",
    "        axes2[j].scatter(range(len(parameter_samples)), parameter_samples, alpha=0.7)\n",
    "        axes2[j].set_title(param_names[j] + ' - Scatter plot')\n",
    "        \n",
    "        # QQ plot\n",
    "        stats.probplot(parameter_samples, dist=\"norm\", plot=axes3[j])\n",
    "        axes3[j].set_title(param_names[j] + ' - QQ plot')\n",
    "\n",
    "    # Set titles for the figures\n",
    "    fig1.suptitle(f'Posterior Distributions for length={t} - Histograms', fontsize=16)\n",
    "    fig2.suptitle(f'Posterior Distributions for length={t} - Scatter Plots', fontsize=16)\n",
    "    fig3.suptitle(f'Posterior Distributions for length={t} - QQ Plots', fontsize=16)\n",
    "\n",
    "    # Adjust layout and save the figures\n",
    "    fig1.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig1.savefig(os.path.join(dir_path_plots, f'histogram_length_{t}.pdf'))\n",
    "    fig2.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig2.savefig(os.path.join(dir_path_plots, f'scatter_length_{t}.pdf'))\n",
    "    fig3.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig3.savefig(os.path.join(dir_path_plots, f'qqplot_length_{t}.pdf'))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timesteps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(dir_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate the statistics for each posterior\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtimesteps\u001b[49m):\n\u001b[0;32m     15\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m posteriors[idx]\n\u001b[0;32m     16\u001b[0m     samples \u001b[38;5;241m=\u001b[39m posterior\u001b[38;5;241m.\u001b[39msample((\u001b[38;5;241m10000\u001b[39m,), show_progress_bars\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'timesteps' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store the results\n",
    "medians = []\n",
    "stds = []\n",
    "kurtoses = []\n",
    "skews = []\n",
    "\n",
    "# Calculate the statistics for each posterior\n",
    "for posterior, t in posteriors:\n",
    "    samples = posterior.sample((10000,), show_progress_bars=False)\n",
    "    samples_np = samples.numpy()\n",
    "    median = np.median(samples_np, axis=0)\n",
    "    std = np.std(samples_np, axis=0)\n",
    "    kurt = kurtosis(samples_np, axis=0)\n",
    "    skewness = skew(samples_np, axis=0)\n",
    "    \n",
    "    medians.append([t] + list(median))\n",
    "    stds.append([t] + list(std)) \n",
    "    kurtoses.append([t] + list(kurt))\n",
    "    skews.append([t] + list(skewness))\n",
    "\n",
    "# Create a DataFrame for each statistic\n",
    "df_medians = pd.DataFrame(medians, columns=['T'] + param_names)\n",
    "df_medians.set_index('T', inplace=True)\n",
    "\n",
    "df_stds = pd.DataFrame(stds, columns=['T'] + param_names)\n",
    "df_stds.set_index('T', inplace=True)\n",
    "\n",
    "df_kurtoses = pd.DataFrame(kurtoses, columns=['T'] + param_names)\n",
    "df_kurtoses.set_index('T', inplace=True)\n",
    "\n",
    "df_skews = pd.DataFrame(skews, columns=['T'] + param_names)\n",
    "df_skews.set_index('T', inplace=True)\n",
    "\n",
    "# Save the DataFrames as CSV files in the specified directory\n",
    "df_medians.to_csv(os.path.join(dir_path_tables, 'medians.csv'))\n",
    "df_stds.to_csv(os.path.join(dir_path_tables, 'standard_deviations.csv'))\n",
    "df_kurtoses.to_csv(os.path.join(dir_path_tables, 'kurtoses.csv'))\n",
    "df_skews.to_csv(os.path.join(dir_path_tables, 'skews.csv'))\n",
    "\n",
    "# Print the tables\n",
    "print(\"Medians:\")\n",
    "print(df_medians)\n",
    "print(\"\\nStandard Deviations:\")\n",
    "print(df_stds)\n",
    "print(\"\\nKurtoses:\")\n",
    "print(df_kurtoses)\n",
    "print(\"\\nSkews:\")\n",
    "print(df_skews)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
